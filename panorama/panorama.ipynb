{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image stitching is one of the most successful applications in\n",
    "Computer Vision. Nowadays, it is hard to find a cell phone or an\n",
    "image processing API that does not contain this\n",
    "functionality(Panorama). This notebook guides you to create panoramic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the images\n",
    "image1 = cv2.imread(r'C:\\Users\\videe\\Downloads\\1.jpeg')\n",
    "image2 = cv2.imread(r'C:\\Users\\videe\\Downloads\\2.jpeg')\n",
    "copy1=image1.copy()\n",
    "copy2=image2.copy()\n",
    "#resize the images with suitable scale\n",
    "image1 = cv2.resize(image1,(0,0), fx=0.5,fy=0.5, interpolation = cv2.INTER_AREA)\n",
    "image2 = cv2.resize(image2,(0,0), fx=0.5,fy=0.5, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert images into grayscale\n",
    "image1_gray=cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "image2_gray=cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create descriptor and detect key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create object\n",
    "orb = cv2.ORB_create()\n",
    "#compute keypoints\n",
    "keypoints1, descriptor1 = orb.detectAndCompute(image1_gray,None)\n",
    "keypoints2, descriptor2 = orb.detectAndCompute(image2_gray,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[174, 112,  74],\n        [177, 115,  77],\n        [177, 115,  77],\n        ...,\n        [191, 129,  83],\n        [192, 127,  83],\n        [192, 126,  85]],\n\n       [[177, 115,  77],\n        [178, 116,  78],\n        [178, 116,  78],\n        ...,\n        [193, 130,  86],\n        [194, 128,  87],\n        [194, 127,  88]],\n\n       [[178, 116,  78],\n        [178, 116,  78],\n        [177, 115,  77],\n        ...,\n        [195, 129,  88],\n        [194, 127,  88],\n        [194, 126,  89]],\n\n       ...,\n\n       [[158, 174, 190],\n        [158, 174, 190],\n        [157, 173, 189],\n        ...,\n        [158, 169, 178],\n        [160, 172, 181],\n        [160, 172, 181]],\n\n       [[154, 172, 189],\n        [153, 171, 188],\n        [152, 170, 187],\n        ...,\n        [134, 151, 156],\n        [135, 151, 156],\n        [135, 151, 156]],\n\n       [[129, 149, 167],\n        [136, 156, 174],\n        [133, 153, 171],\n        ...,\n        [165, 181, 181],\n        [162, 178, 178],\n        [162, 178, 178]]], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#draw Keypoints\n",
    "#use cv2.drawKeypoints()\n",
    "keypoints_without_size = np.copy(image1)\n",
    "keypoints_with_size = np.copy(image1)\n",
    "\n",
    "cv2.drawKeypoints(image1, keypoints1, keypoints_without_size, color = (0, 255, 0))\n",
    "\n",
    "cv2.drawKeypoints(image1, keypoints1, keypoints_with_size, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.imshow('image1',copy1)\n",
    "cv2.imshow('image2',copy2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references:- https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a bruteforce matcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "#match the descriptors using bf.match() \n",
    "matches=bf.match(descriptor1,descriptor2)\n",
    "#sort matches\n",
    "matches=sorted(matches, key = lambda x : x.distance)\n",
    "#Select first few matches from sorted matches\n",
    "#because having a large number of matches cause RANSAC algorithm to overfit the data\n",
    "matches=matches[:int(len(matches)*0.3)]\n",
    "#draw matches\n",
    "result=cv2.drawMatches(image1,keypoints1,image2_gray,keypoints2,matches,image2_gray, flags = 2)\n",
    "\n",
    "cv2.imshow(\"result\",result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of matches = bf.match(des1,des2) line is a list of DMatch objects. This DMatch object has following attributes:\n",
    "\n",
    "DMatch.distance - Distance between descriptors. The lower, the better it is.\n",
    "DMatch.trainIdx - Index of the descriptor in train descriptors\n",
    "DMatch.queryIdx - Index of the descriptor in query descriptors\n",
    "DMatch.imgIdx - Index of the train image.\n",
    "\n",
    "So, Now we need to extract the points and feed them to cv2.findhomography function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finding homography\n",
    "\n",
    "points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "h,mask = cv2.findHomography(points1,points2,cv2.RANSAC,5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wrap the image\n",
    "dst = cv2.warpPerspective(image1,h,(image1.shape[1] + image2.shape[0], image2.shape[0]))\n",
    "cv2.imshow('Wrapped',dst)\n",
    "dst[0:image2.shape[0], 0:image2.shape[1]] = image2\n",
    "cv2.imshow('stitched',dst)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bitfed74f71efd04d719732af73290f909f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}